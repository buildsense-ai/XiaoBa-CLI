# XiaoBa 系统架构优化方案

> 本文档聚焦系统架构层面的问题与优化建议，不涉及 Skill 内容设计。
> 基于对 `src/commands/chat.ts`、`src/utils/ai-service.ts`、`src/tools/tool-manager.ts`、`src/agents/base-agent.ts` 等核心模块的源码审查。

---

## 一、问题总览

| 优先级 | 问题 | 所在文件 | 影响范围 |
|--------|------|----------|----------|
| **P0** | 工具调用循环重复 3 次 | `chat.ts` | 可维护性 |
| **P0** | contextMessages / conversationHistory 分裂 | `chat.ts` | 上下文完整性 |
| **P0** | 无 Streaming 输出 | `ai-service.ts` | 用户体验 |
| **P1** | ToolExecutionContext.conversationHistory 永远为空 | `tool-manager.ts` | 工具能力 |
| **P1** | process.exit 覆盖 + setInterval 保活 | `chat.ts` | 进程稳定性 |
| **P1** | 用 axios 手动调用而非官方 SDK | `ai-service.ts` | 可靠性 |
| **P2** | 多工具调用串行执行 | `tool-manager.ts` | 性能 |
| **P2** | max_tokens 硬编码 4096 | `ai-service.ts` | 输出质量 |
| **P2** | 无依赖注入，不可测试 | 全局 | 工程质量 |
| **P3** | Agent 模型映射硬编码 | `base-agent.ts` | 维护成本 |
| **P3** | 历史压缩机制粗糙 | `chat.ts` | 长对话质量 |

---

## 二、P0 — 必须立即修复

### 2.1 工具调用循环重复 3 次

**现状**

`chat.ts` 中存在三段几乎完全相同的 `while(true)` 工具调用循环：

| 位置 | 函数 | 行号 |
|------|------|------|
| 第 1 处 | `sendSingleMessage()` | 115-170 |
| 第 2 处 | `interactiveChat()` line handler | 394-449 |
| 第 3 处 | `handleSlashCommand()` | 606-661 |

每处都包含：发送 AI 请求 → 检查 toolCalls → 执行工具 → 追加结果 → 循环。逻辑完全一致，连 `task_planner` 的特殊显示逻辑都复制了三遍。

**风险**

- 修改一处忘改另外两处，产生行为不一致
- 新增工具的特殊处理逻辑需要改三个地方
- 代码膨胀，chat.ts 已达 705 行

**建议方案**

抽取 `ConversationRunner` 类，封装核心对话循环：

```typescript
// src/core/conversation-runner.ts
class ConversationRunner {
  constructor(
    private aiService: AIService,
    private toolManager: ToolManager,
    private options?: { onToolStart?: (name: string) => void }
  ) {}

  async run(messages: Message[]): Promise<{ response: string; messages: Message[] }> {
    const tools = this.toolManager.getToolDefinitions();
    while (true) {
      const result = await this.aiService.chat(messages, tools);
      if (!result.toolCalls?.length) {
        return { response: result.content || '', messages };
      }
      messages.push({ role: 'assistant', content: result.content, tool_calls: result.toolCalls });
      for (const toolCall of result.toolCalls) {
        this.options?.onToolStart?.(toolCall.function.name);
        const toolResult = await this.toolManager.executeTool(toolCall);
        messages.push({ role: 'tool', content: toolResult.content, tool_call_id: toolResult.tool_call_id, name: toolResult.name });
      }
    }
  }
}
```

三处调用统一改为：

```typescript
const runner = new ConversationRunner(aiService, toolManager, {
  onToolStart: (name) => Logger.info(`执行工具: ${name}`)
});
const { response, messages } = await runner.run(contextMessages);
```

---

### 2.2 contextMessages 与 conversationHistory 分裂

**现状**

`interactiveChat()` 中（`chat.ts:369`）：

```typescript
let contextMessages: Message[] = [...conversationHistory];
```

后续工具调用的中间消息（assistant with tool_calls、tool results）只追加到 `contextMessages`，**不会回写到 `conversationHistory`**。只有最终的纯文本回复才加入 history（`chat.ts:453`）。

**数据流示意**

```
用户输入 "分析这个文件"
    │
    ├─→ conversationHistory: [system, user:"分析这个文件"]
    ├─→ contextMessages:     [system, user:"分析这个文件"]
    │
    │   AI 返回 tool_call: read_file
    │   contextMessages += [assistant{tool_calls}, tool{result}]  ← 只加到这里
    │   conversationHistory 不变
    │
    │   AI 返回最终文本 "分析结果是..."
    │   conversationHistory += [assistant:"分析结果是..."]  ← 只加最终文本
    │
    └─→ 结果：工具调用过程从 conversationHistory 中完全消失
```

**后果**

1. 历史压缩时丢失工具交互上下文，摘要质量下降
2. 记忆系统永远看不到工具调用过程
3. 下一轮对话中，AI 不知道自己上一轮调用了什么工具、看到了什么结果

**建议方案**

将工具调用过程也同步到 `conversationHistory`，或者直接废弃 `contextMessages`，统一使用一个消息列表。记忆上下文（memory context）可以作为临时消息插入但标记为 ephemeral，压缩时跳过。

---

### 2.3 无 Streaming 输出

**现状**

`ai-service.ts` 的 `chatWithAnthropic()` 和 `chatWithOpenAI()` 都使用普通 HTTP POST，等待完整响应返回后才处理。用户在整个过程中只能看到 spinner 转圈。

```
用户输入 → [spinner 转 5-30 秒] → 一次性输出全部文本
```

**影响**

- CLI 工具的核心体验指标是"首字节延迟"（Time to First Token），当前为 0 — 用户完全无感知
- 长回复（论文分析、代码生成）等待时间可达 30 秒以上，用户无法判断是卡死还是在生成
- 工具调用密集时，多轮 AI 请求串联，总等待时间成倍增长

**建议方案**

使用官方 SDK 的 streaming API：

```typescript
// Anthropic streaming
import Anthropic from '@anthropic-ai/sdk';

const client = new Anthropic({ apiKey, baseURL });
const stream = client.messages.stream({
  model, messages, tools, max_tokens
});

// 逐 token 输出文本
stream.on('text', (text) => process.stdout.write(text));

// 捕获工具调用
const finalMessage = await stream.finalMessage();
```

```typescript
// OpenAI streaming
const stream = await openai.chat.completions.create({
  model, messages, tools, stream: true
});

for await (const chunk of stream) {
  const delta = chunk.choices[0]?.delta;
  if (delta?.content) process.stdout.write(delta.content);
}
```

Streaming 改造需要同步调整 `ConversationRunner`，使其支持流式回调。

---

## 三、P1 — 重要问题

### 3.1 ToolExecutionContext.conversationHistory 永远为空

**现状**

`tool-manager.ts:116-118`：

```typescript
const context: ToolExecutionContext = {
  workingDirectory: this.workingDirectory,
  conversationHistory: []  // 硬编码空数组
};
```

所有工具执行时拿到的对话历史都是空的。

**影响**

- `TaskTool` 无法将父对话上下文传递给子 Agent
- `SkillTool` 无法根据当前对话状态决定行为
- 任何需要"理解当前在做什么"的工具都无法工作

**建议方案**

`ToolManager.executeTool()` 增加可选的 `messages` 参数：

```typescript
async executeTool(toolCall: ToolCall, messages?: Message[]): Promise<ToolResult> {
  const context: ToolExecutionContext = {
    workingDirectory: this.workingDirectory,
    conversationHistory: messages || []
  };
  // ...
}
```

调用方（`ConversationRunner`）在执行工具时传入当前消息列表。

---

### 3.2 process.exit 覆盖 + setInterval 保活

**现状**

`chat.ts:250-281` 覆盖了全局 `process.exit`，用 `setInterval(() => {}, 100)` 阻止进程退出，等待异步清理（记忆压缩）完成。`chat.ts:284-315` 的 SIGINT handler 包含几乎完全相同的逻辑。

```typescript
// chat.ts:250 — 覆盖 process.exit
(process.exit as any) = (code?: number) => {
  // ...
  const keepAliveTimer = setInterval(() => {}, 100);  // 保活 hack
  const cleanup = async () => { /* ... */ };
  cleanup();
};
```

**风险**

- 如果 `compressHistory` 中的网络请求超时或挂起，进程永远不会退出
- `process.exit` 覆盖是全局副作用，影响所有模块（包括第三方库）
- 退出逻辑重复两遍（process.exit 覆盖 + SIGINT handler）
- 没有超时保护机制

**建议方案**

用 `AbortController` + 超时兜底替代：

```typescript
async function gracefulShutdown(conversationHistory: Message[], memoryService: GauzMemService | null) {
  const controller = new AbortController();
  const timeout = setTimeout(() => controller.abort(), 5000); // 5 秒超时

  try {
    if (conversationHistory.length > 0 && memoryService) {
      await compressHistory(conversationHistory, aiService, memoryService);
    }
  } catch (e) {
    // 超时或失败，静默处理
  } finally {
    clearTimeout(timeout);
    process.exit(0);
  }
}

rl.on('close', () => gracefulShutdown(conversationHistory, memoryService));
process.on('SIGINT', () => { rl.close(); });
```

不再覆盖 `process.exit`，不再使用 `setInterval` 保活。

---

### 3.3 用 axios 手动调用而非官方 SDK

**现状**

`package.json` 已声明依赖 `@anthropic-ai/sdk: ^0.27.0`，但 `ai-service.ts` 完全使用 axios 手动构造 HTTP 请求：

- 手动拼接 URL（`ai-service.ts:139-141`）
- 手动设置 headers（`ai-service.ts:147-151`）
- 手动解析响应格式（`ai-service.ts:156-178`）
- API 版本号硬编码 `anthropic-version: '2023-06-01'`

**缺失能力**

| 能力 | axios 手动调用 | 官方 SDK |
|------|---------------|----------|
| Streaming | 不支持 | 内置 |
| 自动重试 | 无 | 内置（指数退避） |
| 速率限制处理 | 无 | 内置 |
| 类型安全 | 全是 `any` | 完整 TypeScript 类型 |
| API 版本管理 | 手动硬编码 | SDK 自动管理 |
| Token 计数 | 无 | 响应中包含 usage |

**建议方案**

用 `@anthropic-ai/sdk` 替换 Anthropic 的 axios 调用，用 `openai` 包替换 OpenAI 的 axios 调用。保留 `AIService` 作为统一抽象层，内部委托给对应 SDK。

---

## 四、P2 — 性能与工程质量

### 4.1 多工具调用串行执行

**现状**

`tool-manager.ts:143-152`：

```typescript
async executeTools(toolCalls: ToolCall[]): Promise<ToolResult[]> {
  const results: ToolResult[] = [];
  for (const toolCall of toolCalls) {
    const result = await this.executeTool(toolCall);  // 逐个等待
    results.push(result);
  }
  return results;
}
```

当 AI 一次返回多个工具调用（如同时读取 3 个文件），它们被串行执行。

**建议方案**

对无副作用的工具（read_file、glob、grep）使用 `Promise.all` 并行执行；对有副作用的工具（write_file、execute_bash）保持串行：

```typescript
async executeTools(toolCalls: ToolCall[]): Promise<ToolResult[]> {
  const readOnlyTools = new Set(['read_file', 'glob', 'grep']);

  // 分组：只读工具并行，其余串行
  const readOnly = toolCalls.filter(tc => readOnlyTools.has(tc.function.name));
  const sideEffect = toolCalls.filter(tc => !readOnlyTools.has(tc.function.name));

  const [readResults, ...sideResults] = await Promise.all([
    Promise.all(readOnly.map(tc => this.executeTool(tc))),
    ...sideEffect.map(tc => this.executeTool(tc))
  ]);

  // 按原始顺序合并结果
  return toolCalls.map(tc => {
    const all = [...readResults, ...sideResults];
    return all.find(r => r.tool_call_id === tc.id)!;
  });
}
```

---

### 4.2 max_tokens 硬编码且不一致

**现状**

- Anthropic: `ai-service.ts:124` 硬编码 `max_tokens: 4096`
- OpenAI: 完全未设置 `max_tokens`

4096 tokens 对于复杂任务（论文分析、长代码生成）明显不够。Claude 3.5 Sonnet 支持 8192 输出，Claude Opus 4 支持更多。这个值不可通过配置修改。

**建议方案**

将 `max_tokens` 加入 `ChatConfig`，提供合理默认值：

```typescript
export interface ChatConfig {
  // ...existing fields
  maxTokens?: number;  // 默认 8192
}
```

---

### 4.3 无依赖注入，不可测试

**现状**

`chatCommand()` 中所有服务直接实例化：

```typescript
const aiService = new AIService();
const toolManager = new ToolManager();
const skillManager = new SkillManager();
```

没有接口抽象，没有注入点。无法 mock `AIService` 来写单元测试，无法在不调用真实 API 的情况下测试对话流程。

**建议方案**

定义接口，通过构造函数注入：

```typescript
interface IAIService {
  chat(messages: Message[], tools?: ToolDefinition[]): Promise<ChatResponse>;
}

interface IToolManager {
  getToolDefinitions(): ToolDefinition[];
  executeTool(toolCall: ToolCall, messages?: Message[]): Promise<ToolResult>;
}

class ConversationRunner {
  constructor(
    private ai: IAIService,
    private tools: IToolManager
  ) {}
}
```

测试时可以传入 mock 实现，不依赖真实 API。

---

## 五、P3 — 可改进项

### 5.1 Agent 模型映射硬编码

**现状**

`base-agent.ts:97-101`：

```typescript
const modelMap: Record<string, string> = {
  sonnet: 'claude-sonnet-4-5-20250929',
  opus: 'claude-opus-4-5-20251101',
  haiku: 'claude-3-5-haiku-20241022'
};
```

问题：
- 模型 ID 随 Anthropic 发版而过时，需要改代码才能更新
- 只对 Anthropic 生效，OpenAI 用户的 Agent 模型切换被忽略
- 散落在 `BaseAgent` 中，不易发现和维护

**建议方案**

集中到配置文件或常量模块：

```typescript
// src/constants/models.ts
export const MODEL_ALIASES: Record<string, Record<string, string>> = {
  anthropic: {
    sonnet: 'claude-sonnet-4-5-20250929',
    opus: 'claude-opus-4-5-20251101',
    haiku: 'claude-3-5-haiku-20241022',
  },
  openai: {
    default: 'gpt-4o',
    fast: 'gpt-4o-mini',
  }
};
```

---

### 5.2 历史压缩机制粗糙

**现状**

`chat.ts:18-19`：

```typescript
const HISTORY_MAX_LENGTH = 20;   // 触发压缩的阈值
const HISTORY_COMPRESS_COUNT = 15; // 每次压缩的消息数量
```

问题：
- **按消息条数而非 token 数压缩**：一条包含整篇论文的 tool result 和一条 "好的" 被同等对待
- **阈值太低**：工具密集型任务一轮就可能产生 10+ 条消息（assistant + tool_call + tool_result × N），20 条阈值意味着可能每 2 轮对话就触发一次压缩
- **压缩丢失消息类型**：`chat.ts:203-205` 把所有非 user 消息都标记为 "AI"，tool result 的结构化信息被扁平化
- **压缩依赖额外 AI 调用**：如果这次调用失败，历史不压缩，内存持续增长

**建议方案**

1. 引入 token 计数（可用 `tiktoken` 或 SDK 返回的 usage 信息），按 token 预算而非消息条数管理
2. 提高阈值或改为动态阈值
3. 压缩时保留消息角色信息，区分 AI 推理和工具输出
4. 压缩失败时做截断兜底（丢弃最早的消息），避免无限增长

---

## 六、重构方案 — 目标架构

### 6.1 模块拆分

当前 `chat.ts` 承担了过多职责。重构后的模块划分：

```
src/
├── core/
│   ├── conversation-runner.ts   ← 新增：核心对话循环（工具调用 loop）
│   ├── history-manager.ts       ← 新增：历史管理（压缩、token 计数）
│   └── stream-handler.ts        ← 新增：streaming 输出处理
├── providers/
│   ├── provider.ts              ← 新增：统一 Provider 接口
│   ├── anthropic-provider.ts    ← 新增：Anthropic SDK 封装
│   └── openai-provider.ts       ← 新增：OpenAI SDK 封装
├── commands/
│   └── chat.ts                  ← 瘦身：只保留 readline + UI 逻辑
├── tools/
│   └── tool-manager.ts          ← 改造：支持上下文传递、并行执行
├── agents/
│   └── base-agent.ts            ← 改造：复用 ConversationRunner
└── utils/
    └── ai-service.ts            ← 改造：委托给 Provider 实现
```

### 6.2 核心类设计

**Provider 接口 — 统一 AI 调用抽象**

```typescript
// src/providers/provider.ts
export interface StreamCallbacks {
  onText?: (text: string) => void;
  onToolCall?: (toolCall: ToolCall) => void;
  onComplete?: (response: ChatResponse) => void;
  onError?: (error: Error) => void;
}

export interface AIProvider {
  chat(messages: Message[], tools?: ToolDefinition[]): Promise<ChatResponse>;
  chatStream(messages: Message[], tools?: ToolDefinition[], callbacks?: StreamCallbacks): Promise<ChatResponse>;
}
```

**ConversationRunner — 核心对话循环**

```typescript
// src/core/conversation-runner.ts
export interface RunnerOptions {
  maxTurns?: number;           // 最大工具调用轮次，默认 30
  stream?: boolean;            // 是否启用 streaming
  onText?: (text: string) => void;
  onToolStart?: (name: string) => void;
  onToolEnd?: (name: string, result: string) => void;
}

export class ConversationRunner {
  constructor(
    private provider: AIProvider,
    private toolManager: IToolManager
  ) {}

  async run(messages: Message[], options?: RunnerOptions): Promise<RunResult> {
    const tools = this.toolManager.getToolDefinitions();
    let turns = 0;

    while (turns++ < (options?.maxTurns ?? 30)) {
      const response = options?.stream
        ? await this.provider.chatStream(messages, tools, { onText: options.onText })
        : await this.provider.chat(messages, tools);

      if (!response.toolCalls?.length) {
        return { response: response.content || '', messages };
      }

      messages.push({
        role: 'assistant',
        content: response.content,
        tool_calls: response.toolCalls
      });

      for (const toolCall of response.toolCalls) {
        options?.onToolStart?.(toolCall.function.name);
        const result = await this.toolManager.executeTool(toolCall, messages);
        messages.push({
          role: 'tool',
          content: result.content,
          tool_call_id: result.tool_call_id,
          name: result.name
        });
        options?.onToolEnd?.(toolCall.function.name, result.content);
      }
    }

    return { response: '[达到最大工具调用轮次]', messages };
  }
}
```

**HistoryManager — 历史管理**

```typescript
// src/core/history-manager.ts
export class HistoryManager {
  private messages: Message[] = [];
  private tokenBudget: number;

  constructor(tokenBudget: number = 100000) {
    this.tokenBudget = tokenBudget;
  }

  push(msg: Message): void {
    this.messages.push(msg);
  }

  getMessages(): Message[] {
    return [...this.messages];
  }

  /** 基于 token 预算判断是否需要压缩 */
  needsCompression(): boolean {
    return this.estimateTokens() > this.tokenBudget * 0.8;
  }

  /** 压缩历史，保留 system prompt + 最近 N 条 */
  async compress(summarizer: (msgs: Message[]) => Promise<string>): Promise<void> {
    const systemMsgs = this.messages.filter(m => m.role === 'system');
    const nonSystem = this.messages.filter(m => m.role !== 'system');

    const toCompress = nonSystem.slice(0, -10); // 保留最近 10 条
    const toKeep = nonSystem.slice(-10);

    if (toCompress.length === 0) return;

    const summary = await summarizer(toCompress);
    this.messages = [
      ...systemMsgs,
      { role: 'system', content: `[历史摘要]\n${summary}` },
      ...toKeep
    ];
  }

  private estimateTokens(): number {
    // 粗略估算：1 token ≈ 1.5 个中文字符 / 4 个英文字符
    return this.messages.reduce((sum, m) => sum + (m.content?.length || 0) / 2, 0);
  }
}
```

### 6.3 重构后的 chat.ts（示意）

重构后 `chat.ts` 只负责 UI 层，核心逻辑全部委托：

```typescript
// src/commands/chat.ts — 重构后约 150 行
export async function chatCommand(options: CommandOptions): Promise<void> {
  // 1. 初始化
  const provider = createProvider(ConfigManager.getConfig());
  const toolManager = new ToolManager();
  const runner = new ConversationRunner(provider, toolManager);
  const history = new HistoryManager();

  // 2. 加载 system prompt
  history.push({ role: 'system', content: await PromptManager.buildSystemPrompt() });

  // 3. 单条消息模式
  if (options.message) {
    history.push({ role: 'user', content: options.message });
    const { response } = await runner.run(history.getMessages(), {
      stream: true,
      onText: (t) => process.stdout.write(t),
      onToolStart: (n) => Logger.info(`执行工具: ${n}`)
    });
    history.push({ role: 'assistant', content: response });
    return;
  }

  // 4. 交互模式 — 只处理 readline + UI
  const rl = readline.createInterface({ input: process.stdin, output: process.stdout });

  rl.on('line', async (input) => {
    if (handleBuiltinCommand(input, history)) { rl.prompt(); return; }

    history.push({ role: 'user', content: input });
    const { response } = await runner.run(history.getMessages(), {
      stream: true,
      onText: (t) => process.stdout.write(t),
      onToolStart: (n) => Logger.info(`执行工具: ${n}`)
    });
    history.push({ role: 'assistant', content: response });

    if (history.needsCompression()) {
      await history.compress((msgs) => summarize(provider, msgs));
    }
    rl.prompt();
  });

  rl.on('close', () => gracefulShutdown(history, memoryService));
  rl.prompt();
}
```

对比当前 705 行的 `chat.ts`，重构后：
- 工具调用循环：3 处 → 0 处（在 `ConversationRunner` 中）
- 退出逻辑：3 处 → 1 处（`gracefulShutdown`）
- 历史管理：散落各处 → `HistoryManager` 集中管理

---

## 七、实施路线图

建议分 4 步推进，每步可独立验证，不需要一次性全改。

### Step 1：抽取 ConversationRunner（消除代码重复）

**改动范围**：新增 `src/core/conversation-runner.ts`，改造 `chat.ts`

**具体步骤**：
1. 创建 `ConversationRunner` 类，将 `chat.ts` 中的 while(true) 工具调用循环提取进去
2. `sendSingleMessage()`、`interactiveChat()` line handler、`handleSlashCommand()` 三处统一调用 `runner.run()`
3. 删除三处重复代码

**验证方式**：功能不变，chat 交互行为与重构前一致

**预期效果**：`chat.ts` 从 705 行降至约 350 行

---

### Step 2：修复历史管理（消除上下文丢失）

**改动范围**：改造 `chat.ts` 历史逻辑，新增 `src/core/history-manager.ts`

**具体步骤**：
1. 创建 `HistoryManager` 类，统一管理消息列表
2. 废弃 `contextMessages` 临时变量，工具调用中间消息统一写入 `HistoryManager`
3. 记忆上下文（memory context）标记为 ephemeral，压缩时跳过
4. `ToolManager.executeTool()` 增加 `messages` 参数，传入当前历史

**验证方式**：
- 执行一轮包含工具调用的对话后，检查 `conversationHistory` 是否包含 tool 消息
- 触发历史压缩后，检查摘要是否包含工具交互信息

---

### Step 3：替换 axios 为官方 SDK + 加入 Streaming

**改动范围**：重写 `ai-service.ts`，新增 `src/providers/`

**具体步骤**：
1. 新建 `AIProvider` 接口，定义 `chat()` 和 `chatStream()` 方法
2. 新建 `AnthropicProvider`，用 `@anthropic-ai/sdk` 实现（已在 package.json 中）
3. 新建 `OpenAIProvider`，用 `openai` 包实现（需新增依赖）
4. `AIService` 改为工厂 + 代理，根据 config.provider 委托给对应 Provider
5. `ConversationRunner` 增加 `stream` 选项，支持逐 token 输出
6. `chat.ts` 的 spinner 替换为 streaming 输出

**验证方式**：
- 发送一条消息，观察是否逐字输出而非一次性显示
- 工具调用场景下，文本部分流式输出，工具执行部分显示状态信息

---

### Step 4：清理退出逻辑 + 工具并行执行

**改动范围**：`chat.ts` 退出逻辑，`tool-manager.ts`

**具体步骤**：
1. 删除 `process.exit` 覆盖和 `setInterval` 保活 hack
2. 用 `rl.on('close')` + `gracefulShutdown()` 统一退出流程，加 5 秒超时
3. `ToolManager.executeTools()` 对只读工具启用 `Promise.all` 并行执行
4. `max_tokens` 加入 `ChatConfig`，默认值提升到 8192

**验证方式**：
- Ctrl+C 退出后进程在 5 秒内正常结束
- 同时读取多个文件时，观察执行时间是否缩短

---

## 八、实施顺序总结

```
Step 1: 抽取 ConversationRunner
  ↓  （消除重复代码，为后续改造打基础）
Step 2: 修复历史管理
  ↓  （消除上下文丢失，提升对话质量）
Step 3: SDK + Streaming
  ↓  （核心体验提升）
Step 4: 退出逻辑 + 并行执行
       （稳定性和性能收尾）
```

每一步都是独立可交付的，不需要等全部完成才能使用。Step 1 是后续所有改动的前提，建议最先执行。
```
